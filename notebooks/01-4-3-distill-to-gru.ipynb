{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD_Data/active_projects/transformer_to_lstm\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import nlp\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_helper_bot import (\n",
    "    BaseBot, MovingAverageStatsTrackerCallback,  CheckpointCallback,\n",
    "    LearningRateSchedulerCallback, MultiStageScheduler, Top1Accuracy,\n",
    "    LinearLR, Callback\n",
    ")\n",
    "\n",
    "from nobita.models import get_sequence_model\n",
    "\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    APEX_AVAILABLE = False\n",
    "# APEX_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"cache/\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "    * https://github.com/huggingface/nlp/blob/master/notebooks/Overview.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, entries_dict):\n",
    "        super().__init__()\n",
    "        self.entries_dict = entries_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.entries_dict[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.entries_dict[\"input_ids\"][idx],\n",
    "            self.entries_dict[\"attention_mask\"][idx].sum(), #input_lengths\n",
    "            {\n",
    "                \"label\": self.entries_dict[\"label\"][idx], \n",
    "                \"logits\": self.entries_dict[\"logits\"][idx]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict, valid_dict, test_dict = torch.load(str(CACHE_DIR / \"distill-dicts.jbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PyTorch Dataloader around our dataset\n",
    "train_loader = torch.utils.data.DataLoader(SST2Dataset(train_dict), batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(SST2Dataset(valid_dict), batch_size=64, drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(SST2Dataset(test_dict), batch_size=64, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.\n",
    "DISTILL_OBJECTIVE = torch.nn.MSELoss()\n",
    "\n",
    "def distill_loss(logits, targets):\n",
    "    distill_part = DISTILL_OBJECTIVE(\n",
    "        logits.reshape(-1), targets[\"logits\"].reshape(-1)\n",
    "    ) / 2\n",
    "    classification_part = F.cross_entropy(\n",
    "        logits, targets[\"label\"]\n",
    "    )\n",
    "    return ALPHA * classification_part + (1-ALPHA) * distill_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained(str(CACHE_DIR / \"sst2_bert_uncased\")).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.bert.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: apex does not support weight dropping\n",
    "model = get_sequence_model(\n",
    "    voc_size=bert_model.bert.embeddings.word_embeddings.weight.shape[0],\n",
    "    emb_size=bert_model.bert.embeddings.word_embeddings.weight.shape[1],\n",
    "    pad_idx = 0,\n",
    "    dropoute = 0,\n",
    "    rnn_hid = 768,\n",
    "    rnn_layers = 2,\n",
    "    bidir = True,\n",
    "    dropouth = 0.25,\n",
    "    dropouti = 0.25,\n",
    "    wdrop = 0,\n",
    "    unit_type = \"gru\",\n",
    "    fcn_layers = [512, 2],\n",
    "    fcn_dropouts = [0.25, 0.25],\n",
    "    use_attention = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceModel(\n",
       "  (embeddings): BasicEmbeddings(\n",
       "    (encoder): Embedding(30522, 768, padding_idx=0)\n",
       "  )\n",
       "  (encoder): RNNStack(\n",
       "    (rnns): ModuleList(\n",
       "      (0): GRU(768, 384, bidirectional=True)\n",
       "      (1): GRU(768, 384, bidirectional=True)\n",
       "    )\n",
       "    (dropouti): LockedDropout()\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout()\n",
       "      (1): LockedDropout()\n",
       "    )\n",
       "  )\n",
       "  (fcn): AttentionFCN(\n",
       "    (attention): Attention(768, return attention=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): LinearBlock(\n",
       "        (lin): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.25, inplace=False)\n",
       "        (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): LinearBlock(\n",
       "        (lin): Linear(in_features=512, out_features=2, bias=True)\n",
       "        (drop): Dropout(p=0.25, inplace=False)\n",
       "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the embedding weights to the LSTM model\n",
    "try:\n",
    "    model.embeddings.encoder.emb.weight.data = bert_model.bert.embeddings.word_embeddings.weight.data\n",
    "except:\n",
    "    model.embeddings.encoder.weight.data = bert_model.bert.embeddings.word_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the embedding layer\n",
    "for param in model.embeddings.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only leaf tensors\n",
    "parameters = [x for x in model.parameters() if x.is_leaf and x.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(parameters, lr=1e-3, betas=(0.8, 0.99))\n",
    "# optimizer = torch.optim.RMSprop(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "if APEX_AVAILABLE:\n",
    "    model, optimizer = amp.initialize(\n",
    "        model, optimizer, opt_level=\"O1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeCallback(Callback):\n",
    "    def on_batch_inputs(self, bot, input_tensors, targets):\n",
    "        input_tensors = [input_tensors[0].transpose(1, 0), input_tensors[1]]\n",
    "        return input_tensors, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillTop1Accuracy(Top1Accuracy):\n",
    "    def __call__(self, truth, pred):\n",
    "        truth = truth[\"label\"]\n",
    "        return super().__call__(truth, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 20:56:59] SEED: 9293\n",
      "[INFO][06/15/2020 20:56:59] # of parameters: 29,156,610\n",
      "[INFO][06/15/2020 20:56:59] # of trainable parameters: 5,715,714\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_loader) * 10\n",
    "\n",
    "checkpoints = CheckpointCallback(\n",
    "    keep_n_checkpoints=1,\n",
    "    checkpoint_dir=CACHE_DIR / \"distill_model_cache/\",\n",
    "    monitor_metric=\"loss\"\n",
    ")\n",
    "lr_durations = [\n",
    "    int(total_steps*0.2),\n",
    "    int(np.ceil(total_steps*0.8))\n",
    "]\n",
    "break_points = [0] + list(np.cumsum(lr_durations))[:-1]\n",
    "callbacks = [\n",
    "    MovingAverageStatsTrackerCallback(\n",
    "        avg_window=len(train_loader) // 8,\n",
    "        log_interval=len(train_loader) // 10\n",
    "    ),\n",
    "    LearningRateSchedulerCallback(\n",
    "        MultiStageScheduler(\n",
    "            [\n",
    "                LinearLR(optimizer, 0.01, lr_durations[0]),\n",
    "                CosineAnnealingLR(optimizer, lr_durations[1])\n",
    "            ],\n",
    "            start_at_epochs=break_points\n",
    "        )\n",
    "    ),\n",
    "    checkpoints,\n",
    "    TransposeCallback()\n",
    "]\n",
    "    \n",
    "bot = BaseBot(\n",
    "    log_dir = CACHE_DIR / \"distill_logs\",\n",
    "    model=model, \n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader, \n",
    "    clip_grad=10.,\n",
    "    optimizer=optimizer, echo=True,\n",
    "    criterion=distill_loss,\n",
    "    callbacks=callbacks,\n",
    "    pbar=False, use_tensorboard=False,\n",
    "    use_amp=APEX_AVAILABLE,\n",
    "    metrics=(DistillTop1Accuracy(),)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 20:56:59] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.8, 0.99)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[INFO][06/15/2020 20:56:59] Batches per epoch: 1053\n",
      "[INFO][06/15/2020 20:56:59] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10530\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/rapids/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 20:57:01] Step   105 | loss 5.88067631 | lr: 5.94e-05 | 0.020s per step\n",
      "[INFO][06/15/2020 20:57:03] Step   210 | loss 4.98701024 | lr: 1.09e-04 | 0.018s per step\n",
      "[INFO][06/15/2020 20:57:05] Step   315 | loss 4.03495032 | lr: 1.58e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:57:07] Step   420 | loss 3.62690906 | lr: 2.08e-04 | 0.022s per step\n",
      "[INFO][06/15/2020 20:57:09] Step   525 | loss 3.37542540 | lr: 2.57e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:57:10] Metrics at step 526:\n",
      "[INFO][06/15/2020 20:57:10] loss: 2.32088318\n",
      "[INFO][06/15/2020 20:57:10] accuracy: 73.62%\n",
      "[INFO][06/15/2020 20:57:12] Step   630 | loss 3.17826421 | lr: 3.06e-04 | 0.021s per step\n",
      "[INFO][06/15/2020 20:57:14] Step   735 | loss 2.99851566 | lr: 3.56e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:57:16] Step   840 | loss 2.81948140 | lr: 4.05e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:18] Step   945 | loss 2.58947242 | lr: 4.54e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:57:20] Step  1050 | loss 2.44451896 | lr: 5.04e-04 | 0.018s per step\n",
      "[INFO][06/15/2020 20:57:20] Metrics at step 1052:\n",
      "[INFO][06/15/2020 20:57:20] loss: 2.27873448\n",
      "[INFO][06/15/2020 20:57:20] accuracy: 79.82%\n",
      "[INFO][06/15/2020 20:57:20] ====================Epoch 2====================\n",
      "[INFO][06/15/2020 20:57:22] Step  1155 | loss 2.24618956 | lr: 5.53e-04 | 0.021s per step\n",
      "[INFO][06/15/2020 20:57:24] Step  1260 | loss 2.11972053 | lr: 6.03e-04 | 0.018s per step\n",
      "[INFO][06/15/2020 20:57:26] Step  1365 | loss 1.95532995 | lr: 6.52e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:28] Step  1470 | loss 1.92536696 | lr: 7.01e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:30] Step  1575 | loss 1.77653099 | lr: 7.51e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:30] Metrics at step 1578:\n",
      "[INFO][06/15/2020 20:57:30] loss: 1.32866578\n",
      "[INFO][06/15/2020 20:57:30] accuracy: 84.63%\n",
      "[INFO][06/15/2020 20:57:32] Step  1680 | loss 1.70451462 | lr: 8.00e-04 | 0.022s per step\n",
      "[INFO][06/15/2020 20:57:34] Step  1785 | loss 1.59562638 | lr: 8.50e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:36] Step  1890 | loss 1.61346729 | lr: 8.99e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:38] Step  1995 | loss 1.63678728 | lr: 9.48e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:40] Step  2100 | loss 1.62897983 | lr: 9.98e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:40] Metrics at step 2104:\n",
      "[INFO][06/15/2020 20:57:40] loss: 1.23670820\n",
      "[INFO][06/15/2020 20:57:40] accuracy: 82.57%\n",
      "[INFO][06/15/2020 20:57:41] ====================Epoch 3====================\n",
      "[INFO][06/15/2020 20:57:42] Step  2205 | loss 1.50901028 | lr: 1.00e-03 | 0.021s per step\n",
      "[INFO][06/15/2020 20:57:44] Step  2310 | loss 1.47661532 | lr: 1.00e-03 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:46] Step  2415 | loss 1.42338616 | lr: 9.98e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:48] Step  2520 | loss 1.42353227 | lr: 9.95e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:50] Step  2625 | loss 1.38666357 | lr: 9.92e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:57:51] Metrics at step 2630:\n",
      "[INFO][06/15/2020 20:57:51] loss: 1.36063523\n",
      "[INFO][06/15/2020 20:57:51] accuracy: 83.26%\n",
      "[INFO][06/15/2020 20:57:52] Step  2730 | loss 1.37680658 | lr: 9.87e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:54] Step  2835 | loss 1.37698538 | lr: 9.83e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:56] Step  2940 | loss 1.27456627 | lr: 9.77e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:57:58] Step  3045 | loss 1.26735401 | lr: 9.71e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:00] Step  3150 | loss 1.25845013 | lr: 9.64e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:00] Metrics at step 3156:\n",
      "[INFO][06/15/2020 20:58:00] loss: 1.17329975\n",
      "[INFO][06/15/2020 20:58:00] accuracy: 84.17%\n",
      "[INFO][06/15/2020 20:58:01] ====================Epoch 4====================\n",
      "[INFO][06/15/2020 20:58:02] Step  3255 | loss 1.15626410 | lr: 9.56e-04 | 0.021s per step\n",
      "[INFO][06/15/2020 20:58:04] Step  3360 | loss 1.17018900 | lr: 9.47e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:06] Step  3465 | loss 1.17701316 | lr: 9.38e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:08] Step  3570 | loss 1.08140205 | lr: 9.28e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:10] Step  3675 | loss 1.06615800 | lr: 9.18e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:11] Metrics at step 3682:\n",
      "[INFO][06/15/2020 20:58:11] loss: 1.51611399\n",
      "[INFO][06/15/2020 20:58:11] accuracy: 83.72%\n",
      "[INFO][06/15/2020 20:58:12] Step  3780 | loss 1.10672743 | lr: 9.07e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:14] Step  3885 | loss 1.02847351 | lr: 8.95e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:16] Step  3990 | loss 1.03468495 | lr: 8.83e-04 | 0.018s per step\n",
      "[INFO][06/15/2020 20:58:18] Step  4095 | loss 1.06155053 | lr: 8.70e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:20] Step  4200 | loss 1.02203384 | lr: 8.56e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:21] Metrics at step 4208:\n",
      "[INFO][06/15/2020 20:58:21] loss: 1.30116256\n",
      "[INFO][06/15/2020 20:58:21] accuracy: 86.24%\n",
      "[INFO][06/15/2020 20:58:21] ====================Epoch 5====================\n",
      "[INFO][06/15/2020 20:58:22] Step  4305 | loss 0.94289660 | lr: 8.42e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:24] Step  4410 | loss 0.93987389 | lr: 8.27e-04 | 0.019s per step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 20:58:26] Step  4515 | loss 0.90879787 | lr: 8.12e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:28] Step  4620 | loss 0.91673689 | lr: 7.97e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:30] Step  4725 | loss 0.92463056 | lr: 7.81e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:58:31] Metrics at step 4734:\n",
      "[INFO][06/15/2020 20:58:31] loss: 1.03975216\n",
      "[INFO][06/15/2020 20:58:31] accuracy: 86.47%\n",
      "[INFO][06/15/2020 20:58:33] Step  4830 | loss 0.90784699 | lr: 7.64e-04 | 0.021s per step\n",
      "[INFO][06/15/2020 20:58:35] Step  4935 | loss 0.83595148 | lr: 7.47e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:37] Step  5040 | loss 0.84228908 | lr: 7.30e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:58:39] Step  5145 | loss 0.84274006 | lr: 7.13e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:58:41] Step  5250 | loss 0.82992634 | lr: 6.95e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:58:41] Metrics at step 5260:\n",
      "[INFO][06/15/2020 20:58:41] loss: 0.93030948\n",
      "[INFO][06/15/2020 20:58:41] accuracy: 85.09%\n",
      "[INFO][06/15/2020 20:58:41] ====================Epoch 6====================\n",
      "[INFO][06/15/2020 20:58:43] Step  5355 | loss 0.73651665 | lr: 6.77e-04 | 0.021s per step\n",
      "[INFO][06/15/2020 20:58:45] Step  5460 | loss 0.72602906 | lr: 6.58e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:58:47] Step  5565 | loss 0.76263421 | lr: 6.39e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:49] Step  5670 | loss 0.74229968 | lr: 6.20e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:58:51] Step  5775 | loss 0.79476594 | lr: 6.01e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:52] Metrics at step 5786:\n",
      "[INFO][06/15/2020 20:58:52] loss: 0.90921773\n",
      "[INFO][06/15/2020 20:58:52] accuracy: 87.61%\n",
      "[INFO][06/15/2020 20:58:53] Step  5880 | loss 0.76777431 | lr: 5.82e-04 | 0.021s per step\n",
      "[INFO][06/15/2020 20:58:55] Step  5985 | loss 0.71119149 | lr: 5.63e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:57] Step  6090 | loss 0.71661145 | lr: 5.43e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:58:59] Step  6195 | loss 0.75153705 | lr: 5.24e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:01] Step  6300 | loss 0.67947818 | lr: 5.04e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:02] Metrics at step 6312:\n",
      "[INFO][06/15/2020 20:59:02] loss: 0.98374231\n",
      "[INFO][06/15/2020 20:59:02] accuracy: 87.61%\n",
      "[INFO][06/15/2020 20:59:02] ====================Epoch 7====================\n",
      "[INFO][06/15/2020 20:59:04] Step  6405 | loss 0.63714821 | lr: 4.84e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:06] Step  6510 | loss 0.59759556 | lr: 4.65e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:08] Step  6615 | loss 0.60106849 | lr: 4.45e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:10] Step  6720 | loss 0.62066227 | lr: 4.26e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:12] Step  6825 | loss 0.62306626 | lr: 4.07e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:12] Metrics at step 6838:\n",
      "[INFO][06/15/2020 20:59:12] loss: 0.93445592\n",
      "[INFO][06/15/2020 20:59:12] accuracy: 87.39%\n",
      "[INFO][06/15/2020 20:59:14] Step  6930 | loss 0.66528959 | lr: 3.87e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:16] Step  7035 | loss 0.63614160 | lr: 3.68e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:18] Step  7140 | loss 0.64591934 | lr: 3.50e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:20] Step  7245 | loss 0.63659821 | lr: 3.31e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:22] Step  7350 | loss 0.61920839 | lr: 3.13e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:23] Metrics at step 7364:\n",
      "[INFO][06/15/2020 20:59:23] loss: 0.92350825\n",
      "[INFO][06/15/2020 20:59:23] accuracy: 87.84%\n",
      "[INFO][06/15/2020 20:59:23] ====================Epoch 8====================\n",
      "[INFO][06/15/2020 20:59:24] Step  7455 | loss 0.58126256 | lr: 2.95e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:26] Step  7560 | loss 0.58778963 | lr: 2.77e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:29] Step  7665 | loss 0.53292904 | lr: 2.60e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:31] Step  7770 | loss 0.53461531 | lr: 2.43e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:33] Step  7875 | loss 0.54760716 | lr: 2.26e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:33] Metrics at step 7890:\n",
      "[INFO][06/15/2020 20:59:33] loss: 0.89243140\n",
      "[INFO][06/15/2020 20:59:33] accuracy: 88.53%\n",
      "[INFO][06/15/2020 20:59:35] Step  7980 | loss 0.55727759 | lr: 2.10e-04 | 0.022s per step\n",
      "[INFO][06/15/2020 20:59:37] Step  8085 | loss 0.51705384 | lr: 1.94e-04 | 0.021s per step\n",
      "[INFO][06/15/2020 20:59:39] Step  8190 | loss 0.51370640 | lr: 1.79e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:41] Step  8295 | loss 0.51741674 | lr: 1.64e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:43] Step  8400 | loss 0.52321449 | lr: 1.50e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:44] Metrics at step 8416:\n",
      "[INFO][06/15/2020 20:59:44] loss: 0.85128891\n",
      "[INFO][06/15/2020 20:59:44] accuracy: 87.39%\n",
      "[INFO][06/15/2020 20:59:44] ====================Epoch 9====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 20:59:45] Step  8505 | loss 0.48967614 | lr: 1.36e-04 | 0.022s per step\n",
      "[INFO][06/15/2020 20:59:47] Step  8610 | loss 0.50541305 | lr: 1.23e-04 | 0.020s per step\n",
      "[INFO][06/15/2020 20:59:49] Step  8715 | loss 0.49594498 | lr: 1.10e-04 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:51] Step  8820 | loss 0.50690323 | lr: 9.85e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:53] Step  8925 | loss 0.46257111 | lr: 8.71e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 20:59:54] Metrics at step 8942:\n",
      "[INFO][06/15/2020 20:59:54] loss: 0.84927264\n",
      "[INFO][06/15/2020 20:59:54] accuracy: 88.30%\n",
      "[INFO][06/15/2020 20:59:56] Step  9030 | loss 0.48975894 | lr: 7.64e-05 | 0.021s per step\n",
      "[INFO][06/15/2020 20:59:58] Step  9135 | loss 0.48621000 | lr: 6.63e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:00] Step  9240 | loss 0.48479956 | lr: 5.69e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:02] Step  9345 | loss 0.46938192 | lr: 4.82e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:04] Step  9450 | loss 0.47725036 | lr: 4.01e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:04] Metrics at step 9468:\n",
      "[INFO][06/15/2020 21:00:04] loss: 0.85131126\n",
      "[INFO][06/15/2020 21:00:04] accuracy: 87.84%\n",
      "[INFO][06/15/2020 21:00:04] ====================Epoch 10====================\n",
      "[INFO][06/15/2020 21:00:06] Step  9555 | loss 0.50905753 | lr: 3.28e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:08] Step  9660 | loss 0.47953966 | lr: 2.62e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:10] Step  9765 | loss 0.49370335 | lr: 2.03e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:12] Step  9870 | loss 0.48327314 | lr: 1.51e-05 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:14] Step  9975 | loss 0.44718493 | lr: 1.07e-05 | 0.020s per step\n",
      "[INFO][06/15/2020 21:00:14] Metrics at step 9994:\n",
      "[INFO][06/15/2020 21:00:14] loss: 0.87217367\n",
      "[INFO][06/15/2020 21:00:14] accuracy: 88.76%\n",
      "[INFO][06/15/2020 21:00:16] Step 10080 | loss 0.46054138 | lr: 7.06e-06 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:18] Step 10185 | loss 0.48016521 | lr: 4.16e-06 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:20] Step 10290 | loss 0.48759379 | lr: 2.02e-06 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:22] Step 10395 | loss 0.49610190 | lr: 6.44e-07 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:24] Step 10500 | loss 0.47823111 | lr: 3.34e-08 | 0.019s per step\n",
      "[INFO][06/15/2020 21:00:24] Metrics at step 10520:\n",
      "[INFO][06/15/2020 21:00:24] loss: 0.85392305\n",
      "[INFO][06/15/2020 21:00:24] accuracy: 88.07%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 21:00:24] Training finished. Best step(s):\n",
      "[INFO][06/15/2020 21:00:24] loss: 0.84927264 @ step 8942\n",
      "[INFO][06/15/2020 21:00:24] accuracy: 88.76% @ step 9994\n"
     ]
    }
   ],
   "source": [
    "print(total_steps)\n",
    "\n",
    "bot.train(\n",
    "    total_steps=total_steps,\n",
    "    checkpoint_interval=len(train_loader) // 2\n",
    ")\n",
    "bot.load_model(checkpoints.best_performers[0][1])\n",
    "checkpoints.remove_checkpoints(keep=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_DIR = CACHE_DIR / \"sst2_bert_uncased\"\n",
    "# TARGET_DIR.mkdir(exist_ok=True)\n",
    "# bot.model.save_pretrained(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': (0.8493302934760347, '0.84933029'),\n",
       " 'accuracy': (-0.8830275229357798, '88.30%')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.eval(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': (0.9584249192421589, '0.95842492'),\n",
       " 'accuracy': (-0.8715596330275229, '87.16%')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.eval(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('rapids': conda)",
   "language": "python",
   "name": "python37664bitrapidsconda05ea436670824d2ebed703c8c53b011e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
