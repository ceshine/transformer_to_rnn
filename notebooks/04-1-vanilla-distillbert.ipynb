{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SEED'] = \"42\"\n",
    "\n",
    "import dataclasses\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import nlp\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    BertForSequenceClassification, \n",
    "    DistilBertConfig,\n",
    "    DistilBertForSequenceClassification\n",
    ")\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_helper_bot import (\n",
    "    BaseBot, MovingAverageStatsTrackerCallback,  CheckpointCallback,\n",
    "    LearningRateSchedulerCallback, MultiStageScheduler, Top1Accuracy,\n",
    "    LinearLR, Callback\n",
    ")\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    APEX_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"cache/\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, entries_dict, temperature=1):\n",
    "        super().__init__()\n",
    "        self.entries_dict = entries_dict\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.entries_dict[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.entries_dict[\"input_ids\"][idx],\n",
    "            self.entries_dict[\"attention_mask\"][idx],\n",
    "            {\n",
    "                \"label\": self.entries_dict[\"label\"][idx], \n",
    "                \"logits\": self.entries_dict[\"logits\"][idx] / self.temperature\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict, valid_dict, test_dict = torch.load(str(CACHE_DIR / \"distill-dicts-augmented.jbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PyTorch Dataloader around our dataset\n",
    "TEMPERATURE = 2.\n",
    "train_loader = torch.utils.data.DataLoader(SST2Dataset(train_dict, temperature=TEMPERATURE), batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(SST2Dataset(valid_dict, temperature=TEMPERATURE), batch_size=64, drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(SST2Dataset(test_dict, temperature=1.), batch_size=64, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0\n",
    "DISTILL_OBJECTIVE = torch.nn.MSELoss()\n",
    "\n",
    "def cross_entropy(logits, targets):\n",
    "    targets = F.softmax(targets, dim=-1)\n",
    "    return -(targets * F.log_softmax(logits, dim=-1)).sum(dim=1).mean()\n",
    "\n",
    "def distill_loss(logits, targets):\n",
    "#     distill_part = F.binary_cross_entropy_with_logits(\n",
    "#         logits[:, 1], targets[\"logits\"][:, 1]\n",
    "#     )\n",
    "    distill_part = cross_entropy(\n",
    "        logits, targets[\"logits\"]\n",
    "    )\n",
    "    classification_part = F.cross_entropy(\n",
    "        logits, targets[\"label\"]\n",
    "    )\n",
    "    return ALPHA * classification_part + (1-ALPHA) * distill_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained(str(CACHE_DIR / \"sst2_bert_uncased\")).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.bert.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig(\n",
    "    vocab_size=30522, \n",
    "    max_position_embeddings=128, \n",
    "    sinusoidal_pos_embds=False, \n",
    "    n_layers=2, n_heads=6, dim=768, \n",
    "    hidden_dim=1536, dropout=0.1, \n",
    "    attention_dropout=0.1, activation='gelu', \n",
    "    initializer_range=0.02, qa_dropout=0.1, \n",
    "    seq_classif_dropout=0.5\n",
    ")\n",
    "distill_bert_model = DistilBertForSequenceClassification(\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.bert.embeddings.position_embeddings.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_bert_model.distilbert.embeddings.word_embeddings.weight.data = bert_model.bert.embeddings.word_embeddings.weight.data\n",
    "distill_bert_model.distilbert.embeddings.position_embeddings.weight.data = bert_model.bert.embeddings.position_embeddings.weight.data[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the embedding layer\n",
    "for param in distill_bert_model.distilbert.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_bert_model =distill_bert_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(distill_bert_model.parameters(), lr=1e-4, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "if APEX_AVAILABLE:\n",
    "    distill_bert_model, optimizer = amp.initialize(\n",
    "        distill_bert_model, optimizer, opt_level=\"O1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillTop1Accuracy(Top1Accuracy):\n",
    "    def __call__(self, truth, pred):\n",
    "        truth = truth[\"label\"]\n",
    "        return super().__call__(truth, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class SST2Bot(BaseBot):\n",
    "    log_dir = CACHE_DIR / \"logs\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.loss_format = \"%.6f\"\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_prediction(output):\n",
    "        return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][07/01/2020 20:13:59] SEED: 42\n",
      "[INFO][07/01/2020 20:13:59] # of parameters: 33,586,946\n",
      "[INFO][07/01/2020 20:13:59] # of trainable parameters: 10,046,210\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_loader) * 5\n",
    "\n",
    "checkpoints = CheckpointCallback(\n",
    "    keep_n_checkpoints=1,\n",
    "    checkpoint_dir=CACHE_DIR / \"distill_model_cache/\",\n",
    "    monitor_metric=\"loss\"\n",
    ")\n",
    "lr_durations = [\n",
    "    int(total_steps*0.2),\n",
    "    int(np.ceil(total_steps*0.8))\n",
    "]\n",
    "break_points = [0] + list(np.cumsum(lr_durations))[:-1]\n",
    "callbacks = [\n",
    "    MovingAverageStatsTrackerCallback(\n",
    "        avg_window=len(train_loader) // 8,\n",
    "        log_interval=len(train_loader) // 10\n",
    "    ),\n",
    "    LearningRateSchedulerCallback(\n",
    "        MultiStageScheduler(\n",
    "            [\n",
    "                LinearLR(optimizer, 0.01, lr_durations[0]),\n",
    "                CosineAnnealingLR(optimizer, lr_durations[1])\n",
    "            ],\n",
    "            start_at_epochs=break_points\n",
    "        )\n",
    "    ),\n",
    "    checkpoints\n",
    "]\n",
    "    \n",
    "bot = SST2Bot(\n",
    "    log_dir = CACHE_DIR / \"distill_logs\",\n",
    "    model=distill_bert_model, \n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader, \n",
    "    clip_grad=10.,\n",
    "    optimizer=optimizer, echo=True,\n",
    "    criterion=distill_loss,\n",
    "    callbacks=callbacks,\n",
    "    pbar=False, use_tensorboard=False,\n",
    "    use_amp=APEX_AVAILABLE,\n",
    "    metrics=(DistillTop1Accuracy(),)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][07/01/2020 20:13:59] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0001\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[INFO][07/01/2020 20:13:59] Batches per epoch: 3157\n",
      "[INFO][07/01/2020 20:13:59] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][07/01/2020 20:14:07] Step   315 | loss 0.676484 | lr: 1.09e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:14:15] Step   630 | loss 0.514955 | lr: 2.08e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:14:22] Step   945 | loss 0.436636 | lr: 3.06e-05 | 0.023s per step\n",
      "[INFO][07/01/2020 20:14:30] Step  1260 | loss 0.425980 | lr: 4.05e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:14:37] Step  1575 | loss 0.414432 | lr: 5.04e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:14:37] Metrics at step 1578:\n",
      "[INFO][07/01/2020 20:14:37] loss: 0.466530\n",
      "[INFO][07/01/2020 20:14:37] accuracy: 83.03%\n",
      "[INFO][07/01/2020 20:14:45] Step  1890 | loss 0.404962 | lr: 6.03e-05 | 0.026s per step\n",
      "[INFO][07/01/2020 20:14:53] Step  2205 | loss 0.394814 | lr: 7.02e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:15:01] Step  2520 | loss 0.390542 | lr: 8.00e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:15:08] Step  2835 | loss 0.378836 | lr: 8.99e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:15:16] Step  3150 | loss 0.371571 | lr: 9.98e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:15:16] Metrics at step 3156:\n",
      "[INFO][07/01/2020 20:15:16] loss: 0.434521\n",
      "[INFO][07/01/2020 20:15:16] accuracy: 83.26%\n",
      "[INFO][07/01/2020 20:15:16] ====================Epoch 2====================\n",
      "[INFO][07/01/2020 20:15:24] Step  3465 | loss 0.365377 | lr: 9.99e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:15:31] Step  3780 | loss 0.355514 | lr: 9.95e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:15:39] Step  4095 | loss 0.354207 | lr: 9.87e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:15:46] Step  4410 | loss 0.350584 | lr: 9.77e-05 | 0.024s per step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][07/01/2020 20:15:54] Step  4725 | loss 0.345210 | lr: 9.63e-05 | 0.023s per step\n",
      "[INFO][07/01/2020 20:15:54] Metrics at step 4734:\n",
      "[INFO][07/01/2020 20:15:54] loss: 0.442633\n",
      "[INFO][07/01/2020 20:15:54] accuracy: 84.17%\n",
      "[INFO][07/01/2020 20:16:01] Step  5040 | loss 0.341085 | lr: 9.47e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:16:09] Step  5355 | loss 0.338211 | lr: 9.28e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:16:16] Step  5670 | loss 0.335036 | lr: 9.06e-05 | 0.023s per step\n",
      "[INFO][07/01/2020 20:16:24] Step  5985 | loss 0.332460 | lr: 8.82e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:16:31] Step  6300 | loss 0.329597 | lr: 8.55e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:16:32] Metrics at step 6312:\n",
      "[INFO][07/01/2020 20:16:32] loss: 0.457789\n",
      "[INFO][07/01/2020 20:16:32] accuracy: 83.49%\n",
      "[INFO][07/01/2020 20:16:32] ====================Epoch 3====================\n",
      "[INFO][07/01/2020 20:16:39] Step  6615 | loss 0.317121 | lr: 8.27e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:16:46] Step  6930 | loss 0.316913 | lr: 7.96e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:16:54] Step  7245 | loss 0.316190 | lr: 7.64e-05 | 0.025s per step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][07/01/2020 20:17:02] Step  7560 | loss 0.313495 | lr: 7.29e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:17:10] Step  7875 | loss 0.312311 | lr: 6.94e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:17:10] Metrics at step 7890:\n",
      "[INFO][07/01/2020 20:17:10] loss: 0.436516\n",
      "[INFO][07/01/2020 20:17:10] accuracy: 82.57%\n",
      "[INFO][07/01/2020 20:17:18] Step  8190 | loss 0.313470 | lr: 6.57e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:17:25] Step  8505 | loss 0.308677 | lr: 6.20e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:17:33] Step  8820 | loss 0.307265 | lr: 5.81e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:17:40] Step  9135 | loss 0.303628 | lr: 5.42e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:17:48] Step  9450 | loss 0.304623 | lr: 5.03e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:17:48] Metrics at step 9468:\n",
      "[INFO][07/01/2020 20:17:48] loss: 0.429833\n",
      "[INFO][07/01/2020 20:17:48] accuracy: 84.86%\n",
      "[INFO][07/01/2020 20:17:49] ====================Epoch 4====================\n",
      "[INFO][07/01/2020 20:17:56] Step  9765 | loss 0.300152 | lr: 4.64e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:18:03] Step 10080 | loss 0.298091 | lr: 4.25e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:18:11] Step 10395 | loss 0.298788 | lr: 3.86e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:18:18] Step 10710 | loss 0.299169 | lr: 3.49e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:18:26] Step 11025 | loss 0.295106 | lr: 3.12e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:18:27] Metrics at step 11046:\n",
      "[INFO][07/01/2020 20:18:27] loss: 0.438964\n",
      "[INFO][07/01/2020 20:18:27] accuracy: 85.55%\n",
      "[INFO][07/01/2020 20:18:34] Step 11340 | loss 0.294205 | lr: 2.76e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:18:42] Step 11655 | loss 0.294659 | lr: 2.42e-05 | 0.025s per step\n",
      "[INFO][07/01/2020 20:18:49] Step 11970 | loss 0.294842 | lr: 2.09e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:18:57] Step 12285 | loss 0.293155 | lr: 1.78e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:19:05] Step 12600 | loss 0.294189 | lr: 1.49e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:19:05] Metrics at step 12624:\n",
      "[INFO][07/01/2020 20:19:05] loss: 0.438095\n",
      "[INFO][07/01/2020 20:19:05] accuracy: 85.09%\n",
      "[INFO][07/01/2020 20:19:06] ====================Epoch 5====================\n",
      "[INFO][07/01/2020 20:19:12] Step 12915 | loss 0.293467 | lr: 1.22e-05 | 0.024s per step\n",
      "[INFO][07/01/2020 20:19:20] Step 13230 | loss 0.291588 | lr: 9.78e-06 | 0.025s per step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][07/01/2020 20:19:28] Step 13545 | loss 0.289868 | lr: 7.58e-06 | 0.024s per step\n",
      "[INFO][07/01/2020 20:19:35] Step 13860 | loss 0.290081 | lr: 5.63e-06 | 0.024s per step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][07/01/2020 20:19:43] Step 14175 | loss 0.289177 | lr: 3.96e-06 | 0.025s per step\n",
      "[INFO][07/01/2020 20:19:44] Metrics at step 14202:\n",
      "[INFO][07/01/2020 20:19:44] loss: 0.441592\n",
      "[INFO][07/01/2020 20:19:44] accuracy: 85.09%\n",
      "[INFO][07/01/2020 20:19:51] Step 14490 | loss 0.288174 | lr: 2.58e-06 | 0.025s per step\n",
      "[INFO][07/01/2020 20:19:59] Step 14805 | loss 0.286645 | lr: 1.48e-06 | 0.024s per step\n",
      "[INFO][07/01/2020 20:20:06] Step 15120 | loss 0.288812 | lr: 6.85e-07 | 0.025s per step\n",
      "[INFO][07/01/2020 20:20:14] Step 15435 | loss 0.290974 | lr: 1.91e-07 | 0.025s per step\n",
      "[INFO][07/01/2020 20:20:22] Step 15750 | loss 0.289079 | lr: 2.01e-09 | 0.025s per step\n",
      "[INFO][07/01/2020 20:20:23] Metrics at step 15780:\n",
      "[INFO][07/01/2020 20:20:23] loss: 0.441398\n",
      "[INFO][07/01/2020 20:20:23] accuracy: 85.09%\n",
      "[INFO][07/01/2020 20:20:23] Training finished. Best step(s):\n",
      "[INFO][07/01/2020 20:20:23] loss: 0.429833 @ step 9468\n",
      "[INFO][07/01/2020 20:20:23] accuracy: 85.55% @ step 11046\n"
     ]
    }
   ],
   "source": [
    "print(total_steps)\n",
    "\n",
    "bot.train(\n",
    "    total_steps=total_steps,\n",
    "    checkpoint_interval=len(train_loader) // 2\n",
    ")\n",
    "bot.load_model(checkpoints.best_performers[0][1])\n",
    "checkpoints.remove_checkpoints(keep=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': (0.4298326703933401, '0.429833'),\n",
       " 'accuracy': (-0.8486238532110092, '84.86%')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.eval(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': (0.3775219143530644, '0.377522'),\n",
       " 'accuracy': (-0.8325688073394495, '83.26%')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.eval(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
